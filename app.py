import streamlit as st
import pandas as pd
import numpy as np
import openai
from sklearn.metrics.pairwise import cosine_similarity
import logging
import json
import re

# --- Configura√ß√£o da P√°gina e Logger ---
st.set_page_config(page_title="Tutor de Matem√°tica", page_icon="ü§ñ", layout="centered")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

# --- INICIALIZA√á√ÉO DO SESSION STATE ---
if "app_state" not in st.session_state:
    st.session_state.app_state = "COLETA_INFO"
if "messages" not in st.session_state:
    st.session_state.messages = []
if "log_messages" not in st.session_state:
    st.session_state.log_messages = []
if "aluno_ano" not in st.session_state:
    st.session_state.aluno_ano = None
if "aluno_nome" not in st.session_state:
    st.session_state.aluno_nome = ""
if "unidade_tematica_atual" not in st.session_state:
    st.session_state.unidade_tematica_atual = ""
if "topico_selecionado_idx" not in st.session_state:
    st.session_state.topico_selecionado_idx = None

# --- Fun√ß√µes Auxiliares (Logging, LaTeX, etc.) ---
def log_to_terminal(message):
    st.session_state.log_messages.append(str(message))
    logging.info(message)

def corrigir_latex_inline(texto: str) -> str:
    pattern = r'(?<!\$)\\(frac|rac)\{([^\}]+)\}\{([^\}]+)\}(?!\$)'
    def normalizar_e_delimitar(match):
        numerador = match.group(2)
        denominador = match.group(3)
        return f"$\\frac{{{numerador}}}{{{denominador}}}$"
    return re.sub(pattern, normalizar_e_delimitar, texto)

# --- Fun√ß√µes de Carregamento de Dados e RAG ---
@st.cache_data
def carregar_dados():
    log_to_terminal("Iniciando carregamento dos dados...")
    try:
        df = pd.read_parquet("dados_curriculares_enriquecidos.parquet")
        matriz_embeddings = np.array(df['embedding'].tolist())
        return df, matriz_embeddings
    except FileNotFoundError:
        st.error("Arquivo 'dados_curriculares_enriquecidos.parquet' n√£o encontrado.")
        return None, None

def gerar_embedding_query(texto, client):
    log_to_terminal(f"Gerando embedding para a query: '{texto}'")
    try:
        response = client.embeddings.create(input=[texto], model="text-embedding-3-large")
        return response.data[0].embedding
    except Exception as e:
        st.error(f"Erro ao gerar embedding: {e}")
        return None

def buscar_conteudo_inicial(query, df, matriz_embeddings):
    # Esta fun√ß√£o faz a busca inicial para definir a "playlist"
    if not query: return None
    log_to_terminal("Buscando conte√∫do inicial...")
    embedding = gerar_embedding_query(query, client)
    if embedding is None: return None
    
    scores = cosine_similarity([embedding], matriz_embeddings)[0]
    df['similaridade'] = scores
    
    # Retorna o √≠ndice do item com maior similaridade
    top_hit_idx = df['similaridade'].idxmax()
    log_to_terminal(f"Melhor resultado inicial encontrado no √≠ndice {top_hit_idx} com score {df.loc[top_hit_idx, 'similaridade']:.4f}")
    return top_hit_idx

# --- NOVA FUN√á√ÉO PARA EXTRAIR DADOS DO TEXTO ---
def extrair_dados_iniciais(texto_usuario, client):
    log_to_terminal("Extraindo dados da primeira mensagem do usu√°rio...")
    prompt_extracao = f"""
    Extraia o nome do aluno, o ano (como um n√∫mero inteiro) e o assunto principal da frase abaixo.
    Responda APENAS com um objeto JSON com as chaves "nome", "ano" e "assunto".
    Frase: "{texto_usuario}"
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt_extracao}],
            response_format={"type": "json_object"}
        )
        dados = json.loads(response.choices[0].message.content)
        log_to_terminal(f"Dados extra√≠dos: {dados}")
        return dados
    except Exception as e:
        log_to_terminal(f"Erro ao extrair dados iniciais: {e}")
        return None

# --- Inicializa√ß√£o ---
try:
    client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
except (KeyError, FileNotFoundError):
    st.error("Chave da API da OpenAI n√£o encontrada. Configure o arquivo .streamlit/secrets.toml")
    st.stop()

df, matriz_embeddings = carregar_dados()
if df is None: st.stop()

# --- Interface Principal e M√°quina de Estados ---
st.title("ü§ñ Tutor Inteligente de Matem√°tica")

# ESTADO 1: COLETA DE INFORMA√á√ïES
if st.session_state.app_state == "COLETA_INFO":
    st.info("üëã Ol√°! Para come√ßarmos, diga seu nome, o ano que voc√™ est√° cursando e o assunto que gostaria de estudar hoje.")
    st.caption("Exemplo: 'Meu nome √© Ana, sou do 8¬∫ ano e quero aprender sobre o teorema de Pit√°goras.'")

    if prompt := st.chat_input("Diga seu nome, ano e assunto..."):
        dados_iniciais = extrair_dados_iniciais(prompt, client)
        if dados_iniciais:
            st.session_state.aluno_nome = dados_iniciais.get("nome", "estudante")
            st.session_state.aluno_ano = dados_iniciais.get("ano", 7) # Padr√£o para 7¬∫ ano se n√£o encontrar
            assunto = dados_iniciais.get("assunto", prompt)

            top_hit_idx = buscar_conteudo_inicial(assunto, df, matriz_embeddings)
            if top_hit_idx is not None:
                st.session_state.unidade_tematica_atual = df.loc[top_hit_idx, "Unidade Tem√°tica"]
                st.session_state.topico_selecionado_idx = top_hit_idx
                st.session_state.app_state = "SELECAO_TOPICO"
                st.rerun()
            else:
                st.error("N√£o consegui identificar um t√≥pico. Pode tentar de novo?")
        else:
            st.error("Desculpe, n√£o consegui entender sua mensagem. Por favor, tente o formato do exemplo.")

# ESTADO 2: SELE√á√ÉO DE T√ìPICO (A "PLAYLIST")
elif st.session_state.app_state == "SELECAO_TOPICO":
    st.markdown(f"### Ol√°, {st.session_state.aluno_nome}!")
    st.markdown(f"Legal! Vamos falar sobre **{st.session_state.unidade_tematica_atual}**. Encontrei estes t√≥picos relacionados no curr√≠culo do seu ano e de anos anteriores. O que mais se parece com o que voc√™ procura est√° marcado.")

    # Filtra o DataFrame pela unidade tem√°tica e ordena
    playlist_df = df[df["Unidade Tem√°tica"] == st.session_state.unidade_tematica_atual].sort_values("Ordem")

    for idx, row in playlist_df.iterrows():
        col1, col2, col3 = st.columns([5, 3, 2])
        with col1:
            st.markdown(f"**{row['Objetos do conhecimento']}**")
            st.caption(f"Ano: {row['Ano']} | Ordem: {row['Ordem']}")
        with col2:
            if idx == st.session_state.topico_selecionado_idx:
                st.success("üìç Assunto mais pr√≥ximo")
        with col3:
            if st.button("Estudar este t√≥pico", key=f"topic_{idx}"):
                # Ao clicar, inicia o chat sobre este t√≥pico
                st.session_state.topico_selecionado_idx = idx
                st.session_state.app_state = "CHAT"
                st.session_state.messages = [
                    {"role": "assistant", "content": json.dumps({
                        "response": [f"Ok! Vamos focar em **{df.loc[idx, 'Objetos do conhecimento']}**. O que especificamente voc√™ gostaria de saber? Me pe√ßa uma explica√ß√£o, exemplos ou exerc√≠cios!"]
                    })}
                ]
                st.rerun()
    st.divider()

# ESTADO 3: CHAT (L√ìGICA ANTERIOR ADAPTADA)
elif st.session_state.app_state == "CHAT":
    # A l√≥gica de renderiza√ß√£o e de chat que j√° t√≠nhamos
    def renderizar_mensagem(message):
        # ... (c√≥digo completo na se√ß√£o abaixo) ...
    def criar_query_contextualizada(historico_mensagens, topico_atual):
        # ... (c√≥digo completo na se√ß√£o abaixo) ...

    # Renderiza o hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            renderizar_mensagem(message)
    
    # L√≥gica de input e resposta do chat
    if prompt := st.chat_input("Pe√ßa uma explica√ß√£o, exemplos ou exerc√≠cios!"):
        # ... (l√≥gica completa na se√ß√£o abaixo) ...
        pass # Placeholder

# --- Bloco final para o Estado de CHAT (para manter a legibilidade) ---
# Cole a l√≥gica da fun√ß√£o de renderiza√ß√£o e o loop de chat aqui

def renderizar_mensagem(message):
    if message["role"] == "user":
        st.markdown(message["content"])
        return
    try:
        data = json.loads(message["content"])
        for line in data.get("response", []):
            st.markdown(corrigir_latex_inline(line), unsafe_allow_html=True)
    except (json.JSONDecodeError, TypeError):
        st.markdown(corrigir_latex_inline(message["content"]), unsafe_allow_html=True)

def criar_query_contextualizada(historico_mensagens: list, topico_atual: str) -> str:
    log_to_terminal("Criando query contextualizada para a busca...")
    mensagens_relevantes = historico_mensagens[-4:]
    contexto_str = " ".join([f"{msg['role']}: {msg['content']}" for msg in mensagens_relevantes])
    query_final = f"Contexto do t√≥pico: {topico_atual}. Conversa recente: {contexto_str}"
    return query_final

if st.session_state.app_state == "CHAT":
    if prompt := st.chat_input("Pe√ßa uma explica√ß√£o, exemplos ou exerc√≠cios!"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                st.session_state.log_messages = []
                log_to_terminal("--- NOVA QUERY (CHAT) ---")

                topico_atual_texto = df.loc[st.session_state.topico_selecionado_idx, 'texto_completo']
                query_para_rag = criar_query_contextualizada(st.session_state.messages, topico_atual_texto)
                log_to_terminal(f"Query Contextualizada para RAG: '{query_para_rag}'")

                # A busca agora √© sempre focada no t√≥pico j√° selecionado, mas a query ajuda a refinar.
                # Para simplificar, vamos usar o texto do t√≥pico principal como contexto fixo nesta fase.
                contexto_curricular = topico_atual_texto
                log_to_terminal(f"\n--- Usando contexto do t√≥pico selecionado (√çndice: {st.session_state.topico_selecionado_idx}) ---")

                system_prompt = f"""
                Voc√™ √© um tutor de matem√°tica. Sua resposta DEVE ser um objeto JSON v√°lido com uma chave "response" contendo uma lista de strings.
                Use Markdown e LaTeX (com $...$) para formatar.

                REGRAS DE FORMATA√á√ÉO:
                1. Blocos matem√°ticos ($$) devem estar em seu pr√≥prio item na lista.
                2. F√≥rmulas inline ($) podem estar no meio de um par√°grafo.
                3. Use espa√ßos antes e depois de blocos matem√°ticos.
                4. N√∫meros mistos: `$1\\frac{{1}}{{4}}$`.

                O aluno est√° estudando o t√≥pico: "{df.loc[st.session_state.topico_selecionado_idx, 'Objetos do conhecimento']}".
                Use o CONTEXTO CURRICULAR abaixo para responder a pergunta dele.
                CONTEXTO CURRICULAR: {contexto_curricular}
                """
                mensagens_para_api = [{"role": "system", "content": system_prompt}] + st.session_state.messages
                
                try:
                    response = client.chat.completions.create(
                        model="gpt-4o-mini", messages=mensagens_para_api,
                        response_format={"type": "json_object"}
                    )
                    resposta_json_str = response.choices[0].message.content
                    log_to_terminal("\n--- RESPOSTA BRUTA DA API (JSON) ---")
                    log_to_terminal(resposta_json_str)
                    st.session_state.messages.append({"role": "assistant", "content": resposta_json_str})
                except Exception as e:
                    st.error(f"Ocorreu um erro com a API da OpenAI: {e}")
                    log_to_terminal(f"ERRO na API de Chat: {e}")
        st.rerun()

# Bloco final para exibir o terminal de debug
with st.sidebar:
    st.header("Debug")
    with st.expander("üîå Terminal de Debug", expanded=True):
        log_container = st.container(height=300)
        log_text = "\n".join(st.session_state.log_messages)
        log_container.text(log_text)
